#!/bin/bash
#SBATCH --job-name=distil
#SBATCH --output=/home/jan.cruz/workspace/slurm-output/distil.%A.txt
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --mem=100G
#SBATCH --cpus-per-task=32
#SBATCH --gres=gpu:1
#SBATCH -p cscc-gpu-p
#SBATCH -q cscc-gpu-qos
#SBATCH --time=12:00:00

hostname
nvidia-smi
#module load nvidia/cuda/11.8

#python /home/jan.cruz/workspace/transformers/examples/research_projects/distillation/scripts/binarized_data.py \
#    --file_path /home/jan.cruz/workspace/distillation/data/train.txt \
#    --tokenizer_type bert \
#    --tokenizer_name /l/users/jan.cruz/pretrained-models/bert-base-multilingual-cased \
#    --dump_file /home/jan.cruz/workspace/distillation/data/binarized_text

#python /home/jan.cruz/workspace/transformers/examples/research_projects/distillation/scripts/token_counts.py \
#    --data_file /home/jan.cruz/workspace/distillation/data/binarized_text.bert-base-multilingual-cased.pickle \
#    --token_counts_dump /home/jan.cruz/workspace/distillation/data/token_counts.bert-base-multilingual-cased.pickle \
#    --vocab_size 119547

python /home/jan.cruz/workspace/transformers/examples/research_projects/distillation/train.py \
    --student_type distilbert \
    --student_config /home/jan.cruz/workspace/transformers/examples/research_projects/distillation/training_configs/distilbert-base-multilingual-cased.json \
    --teacher_type bert \
    --teacher_name /l/users/jan.cruz/pretrained-models/bert-base-multilingual-cased \
    --alpha_ce 5.0 --alpha_mlm 2.0 --alpha_cos 1.0 --alpha_clm 0.0 --mlm \
    --batch_size 1 \
    --gradient_accumulation_steps 128 \
    --freeze_pos_embs \
    --dump_path /home/jan.cruz/workspace/distillation/distilbert-base-multilingual-tagalog-cased \
    --data_file /home/jan.cruz/workspace/distillation/data/binarized_text.bert-base-multilingual-cased.pickle \
    --token_counts /home/jan.cruz/workspace/distillation/data/token_counts.bert-base-multilingual-cased.pickle \
    --force # overwrites the `dump_path` if it already exists.